{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7443df46",
      "metadata": {
        "origin_pos": 1,
        "id": "7443df46"
      },
      "source": [
        "# Parameter Management\n",
        "\n",
        "Once we have chosen an architecture\n",
        "and set our hyperparameters,\n",
        "we proceed to the training loop,\n",
        "where our goal is to find parameter values\n",
        "that minimize our loss function.\n",
        "After training, we will need these parameters\n",
        "in order to make future predictions.\n",
        "Additionally, we will sometimes wish\n",
        "to extract the parameters\n",
        "perhaps to reuse them in some other context,\n",
        "to save our model to disk so that\n",
        "it may be executed in other software,\n",
        "or for examination in the hope of\n",
        "gaining scientific understanding.\n",
        "\n",
        "Most of the time, we will be able\n",
        "to ignore the nitty-gritty details\n",
        "of how parameters are declared\n",
        "and manipulated, relying on deep learning frameworks\n",
        "to do the heavy lifting.\n",
        "However, when we move away from\n",
        "stacked architectures with standard layers,\n",
        "we will sometimes need to get into the weeds\n",
        "of declaring and manipulating parameters.\n",
        "In this section, we cover the following:\n",
        "\n",
        "* Accessing parameters for debugging, diagnostics, and visualizations.\n",
        "* Sharing parameters across different model components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "41cbf7e2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:27:20.807089Z",
          "iopub.status.busy": "2023-08-18T19:27:20.806426Z",
          "iopub.status.idle": "2023-08-18T19:27:22.457089Z",
          "shell.execute_reply": "2023-08-18T19:27:22.456154Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "41cbf7e2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "293084ba",
      "metadata": {
        "origin_pos": 6,
        "id": "293084ba"
      },
      "source": [
        "(**We start by focusing on an MLP with one hidden layer.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "9aa0461f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:27:22.461279Z",
          "iopub.status.busy": "2023-08-18T19:27:22.460607Z",
          "iopub.status.idle": "2023-08-18T19:27:22.494399Z",
          "shell.execute_reply": "2023-08-18T19:27:22.493545Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "9aa0461f",
        "outputId": "ccf65570-63e4-48e3-ab3d-547a068d279f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(8),\n",
        "                    nn.ReLU(),\n",
        "                    nn.LazyLinear(1))\n",
        "\n",
        "X = torch.rand(size=(2, 4))\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d0bb094",
      "metadata": {
        "origin_pos": 11,
        "id": "8d0bb094"
      },
      "source": [
        "## [**Parameter Access**]\n",
        ":label:`subsec_param-access`\n",
        "\n",
        "Let's start with how to access parameters\n",
        "from the models that you already know.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81e03323",
      "metadata": {
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "81e03323"
      },
      "source": [
        "When a model is defined via the `Sequential` class,\n",
        "we can first access any layer by indexing\n",
        "into the model as though it were a list.\n",
        "Each layer's parameters are conveniently\n",
        "located in its attribute.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f41ac20",
      "metadata": {
        "origin_pos": 14,
        "id": "5f41ac20"
      },
      "source": [
        "We can inspect the parameters of the second fully connected layer as follows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3c6fdb60",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:27:22.497996Z",
          "iopub.status.busy": "2023-08-18T19:27:22.497442Z",
          "iopub.status.idle": "2023-08-18T19:27:22.504291Z",
          "shell.execute_reply": "2023-08-18T19:27:22.503521Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "3c6fdb60",
        "outputId": "95724b42-49d7-4de5-effc-4e8b330d1ed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[-0.2202,  0.2929,  0.2868, -0.3273, -0.2131, -0.0794,  0.1848, -0.0617]])),\n",
              "             ('bias', tensor([0.1436]))])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "net[2].state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc9e6e49",
      "metadata": {
        "origin_pos": 19,
        "id": "cc9e6e49"
      },
      "source": [
        "We can see that this fully connected layer\n",
        "contains two parameters,\n",
        "corresponding to that layer's\n",
        "weights and biases, respectively.\n",
        "\n",
        "\n",
        "### [**Targeted Parameters**]\n",
        "\n",
        "Note that each parameter is represented\n",
        "as an instance of the parameter class.\n",
        "To do anything useful with the parameters,\n",
        "we first need to access the underlying numerical values.\n",
        "There are several ways to do this.\n",
        "Some are simpler while others are more general.\n",
        "The following code extracts the bias\n",
        "from the second neural network layer, which returns a parameter class instance, and\n",
        "further accesses that parameter's value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ba2da7b4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:27:22.507849Z",
          "iopub.status.busy": "2023-08-18T19:27:22.507181Z",
          "iopub.status.idle": "2023-08-18T19:27:22.513236Z",
          "shell.execute_reply": "2023-08-18T19:27:22.512406Z"
        },
        "origin_pos": 21,
        "tab": [
          "pytorch"
        ],
        "id": "ba2da7b4",
        "outputId": "30e2637c-8d3b-4bc1-f24a-a1c1ffe34eaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.nn.parameter.Parameter, tensor([0.1436]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "type(net[2].bias), net[2].bias.data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c10cf6c",
      "metadata": {
        "origin_pos": 24,
        "tab": [
          "pytorch"
        ],
        "id": "8c10cf6c"
      },
      "source": [
        "Parameters are complex objects,\n",
        "containing values, gradients,\n",
        "and additional information.\n",
        "That is why we need to request the value explicitly.\n",
        "\n",
        "In addition to the value, each parameter also allows us to access the gradient. Because we have not invoked backpropagation for this network yet, it is in its initial state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "4c5f0ae9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:27:22.516723Z",
          "iopub.status.busy": "2023-08-18T19:27:22.516170Z",
          "iopub.status.idle": "2023-08-18T19:27:22.521606Z",
          "shell.execute_reply": "2023-08-18T19:27:22.520790Z"
        },
        "origin_pos": 27,
        "tab": [
          "pytorch"
        ],
        "id": "4c5f0ae9",
        "outputId": "dac9aab7-de0a-454c-a301-f9add5ac3f02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "net[2].weight.grad == None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49d744bc",
      "metadata": {
        "origin_pos": 28,
        "id": "49d744bc"
      },
      "source": [
        "### [**All Parameters at Once**]\n",
        "\n",
        "When we need to perform operations on all parameters,\n",
        "accessing them one-by-one can grow tedious.\n",
        "The situation can grow especially unwieldy\n",
        "when we work with more complex, e.g., nested, modules,\n",
        "since we would need to recurse\n",
        "through the entire tree to extract\n",
        "each sub-module's parameters. Below we demonstrate accessing the parameters of all layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "dab1b4b5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:27:22.525019Z",
          "iopub.status.busy": "2023-08-18T19:27:22.524380Z",
          "iopub.status.idle": "2023-08-18T19:27:22.530002Z",
          "shell.execute_reply": "2023-08-18T19:27:22.529195Z"
        },
        "origin_pos": 30,
        "tab": [
          "pytorch"
        ],
        "id": "dab1b4b5",
        "outputId": "ba8ce900-1f44-4cb4-f8ad-4c1abb1045da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('0.weight', torch.Size([8, 4])),\n",
              " ('0.bias', torch.Size([8])),\n",
              " ('2.weight', torch.Size([1, 8])),\n",
              " ('2.bias', torch.Size([1]))]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "[(name, param.shape) for name, param in net.named_parameters()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd29a2e",
      "metadata": {
        "origin_pos": 33,
        "id": "1fd29a2e"
      },
      "source": [
        "## [**Tied Parameters**]\n",
        "\n",
        "Often, we want to share parameters across multiple layers.\n",
        "Let's see how to do this elegantly.\n",
        "In the following we allocate a fully connected layer\n",
        "and then use its parameters specifically\n",
        "to set those of another layer.\n",
        "Here we need to run the forward propagation\n",
        "`net(X)` before accessing the parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "5b706636",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:27:22.533421Z",
          "iopub.status.busy": "2023-08-18T19:27:22.532786Z",
          "iopub.status.idle": "2023-08-18T19:27:22.541856Z",
          "shell.execute_reply": "2023-08-18T19:27:22.541011Z"
        },
        "origin_pos": 35,
        "tab": [
          "pytorch"
        ],
        "id": "5b706636",
        "outputId": "adc085a5-37c6-42a8-c3dc-ca7a58f8c815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True, True, True, True, True, True, True])\n",
            "tensor([True, True, True, True, True, True, True, True])\n"
          ]
        }
      ],
      "source": [
        "# We need to give the shared layer a name so that we can refer to its\n",
        "# parameters\n",
        "shared = nn.LazyLinear(8)\n",
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    nn.LazyLinear(1))\n",
        "\n",
        "net(X)\n",
        "# Check whether the parameters are the same\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
        "net[2].weight.data[0, 0] = 100\n",
        "# Make sure that they are actually the same object rather than just having the\n",
        "# same value\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec93f84",
      "metadata": {
        "origin_pos": 38,
        "id": "6ec93f84"
      },
      "source": [
        "This example shows that the parameters\n",
        "of the second and third layer are tied.\n",
        "They are not just equal, they are\n",
        "represented by the same exact tensor.\n",
        "Thus, if we change one of the parameters,\n",
        "the other one changes, too.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a7b800f",
      "metadata": {
        "origin_pos": 39,
        "tab": [
          "pytorch"
        ],
        "id": "2a7b800f"
      },
      "source": [
        "You might wonder,\n",
        "when parameters are tied\n",
        "what happens to the gradients?\n",
        "Since the model parameters contain gradients,\n",
        "the gradients of the second hidden layer\n",
        "and the third hidden layer are added together\n",
        "during backpropagation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f0f0ad",
      "metadata": {
        "origin_pos": 40,
        "id": "b4f0f0ad"
      },
      "source": [
        "## Summary\n",
        "\n",
        "We have several ways of accessing and tying model parameters.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Use the `NestMLP` model defined in :numref:`sec_model_construction` and access the parameters of the various layers.\n",
        "1. Construct an MLP containing a shared parameter layer and train it. During the training process, observe the model parameters and gradients of each layer.\n",
        "1. Why is sharing parameters a good idea?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3987814f",
      "metadata": {
        "origin_pos": 42,
        "tab": [
          "pytorch"
        ],
        "id": "3987814f"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/57)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Exercise 1\n",
        "class NestMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
        "                                 nn.LazyLinear(32), nn.ReLU())\n",
        "        self.linear = nn.LazyLinear(16)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.linear(self.net(X))\n",
        "\n",
        "net = NestMLP()\n",
        "blah = net(torch.rand((2,20)))\n",
        "layer_idx = 0\n",
        "for name, module in net.named_modules():\n",
        "  if isinstance(module, (nn.Linear, nn.LazyLinear)):\n",
        "    print(f\"Layer {layer_idx} ({name}):\")\n",
        "    print(f\"  weight: {module.weight.data}\")\n",
        "    print(f\"  bias:   {module.bias.data}\")\n",
        "    layer_idx += 1"
      ],
      "metadata": {
        "id": "KwzV1aUJjNgh",
        "outputId": "15536bfb-eeea-49f0-8017-4f05a5f88b49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KwzV1aUJjNgh",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 (net.0):\n",
            "  weight: tensor([[-0.0413,  0.2025, -0.1952,  ..., -0.1712, -0.1561, -0.1739],\n",
            "        [-0.0292, -0.1820,  0.1680,  ..., -0.2107, -0.0515, -0.1078],\n",
            "        [-0.1216,  0.1733, -0.1661,  ..., -0.2079, -0.1591,  0.1051],\n",
            "        ...,\n",
            "        [-0.0393, -0.1211,  0.1198,  ..., -0.0392,  0.0152, -0.0955],\n",
            "        [ 0.0720,  0.1806, -0.1029,  ...,  0.0282,  0.0698, -0.2081],\n",
            "        [ 0.0192,  0.1625, -0.0339,  ...,  0.1951,  0.1769, -0.0451]])\n",
            "  bias:   tensor([-0.1494, -0.0784, -0.1820,  0.1754, -0.1542, -0.1477,  0.0380, -0.0737,\n",
            "        -0.1642, -0.1109, -0.1341, -0.1247,  0.1313,  0.0531, -0.1253,  0.0227,\n",
            "        -0.0588, -0.0126,  0.0402, -0.0265, -0.2232, -0.1015,  0.0577, -0.1447,\n",
            "        -0.2141,  0.0026, -0.0400,  0.2079, -0.1236,  0.2024,  0.0444, -0.0209,\n",
            "        -0.0664, -0.1077, -0.2127, -0.0584,  0.0626, -0.0530,  0.1068, -0.0477,\n",
            "        -0.1278,  0.0758,  0.1456,  0.1988,  0.0300,  0.1091, -0.0805,  0.0616,\n",
            "         0.1599,  0.1833, -0.1307, -0.1655,  0.1136,  0.0252,  0.0972, -0.1524,\n",
            "        -0.0647, -0.1584, -0.1753, -0.2108, -0.2073,  0.0870, -0.1378, -0.1729])\n",
            "Layer 1 (net.2):\n",
            "  weight: tensor([[-0.0180,  0.0289, -0.1006,  ...,  0.0136, -0.0076, -0.0361],\n",
            "        [-0.0793,  0.0492, -0.0006,  ...,  0.0908,  0.0037,  0.0597],\n",
            "        [ 0.1217,  0.1205,  0.0380,  ...,  0.1198,  0.0252,  0.0784],\n",
            "        ...,\n",
            "        [ 0.1219,  0.0260, -0.0647,  ..., -0.0398,  0.0258,  0.0238],\n",
            "        [-0.0180,  0.0327,  0.0404,  ...,  0.0667,  0.0838,  0.0638],\n",
            "        [ 0.0145,  0.0507,  0.0213,  ...,  0.0645, -0.0065,  0.0210]])\n",
            "  bias:   tensor([-0.1051,  0.0468, -0.1169, -0.0097, -0.0516,  0.0593, -0.0109, -0.0049,\n",
            "         0.1193,  0.0301,  0.1085,  0.0892, -0.0573,  0.0366, -0.0293,  0.1236,\n",
            "         0.0585, -0.0216,  0.0123, -0.0972, -0.1126, -0.0071,  0.1181, -0.1164,\n",
            "         0.0885, -0.0161,  0.1175,  0.1165,  0.0312, -0.0565, -0.0744,  0.0158])\n",
            "Layer 2 (linear):\n",
            "  weight: tensor([[-0.0386,  0.1543, -0.1491,  0.0313,  0.0911, -0.1163, -0.0501, -0.1475,\n",
            "         -0.1099, -0.0504,  0.0377,  0.0670, -0.1270, -0.0700, -0.0835, -0.0506,\n",
            "          0.0261,  0.1138, -0.0921,  0.1119, -0.1456,  0.0696, -0.1218,  0.1236,\n",
            "         -0.0141,  0.1755,  0.1361, -0.0994, -0.0808, -0.1560,  0.1284,  0.1622],\n",
            "        [ 0.1502,  0.1645,  0.1188,  0.1037, -0.1311,  0.0114,  0.1380, -0.0183,\n",
            "         -0.1246, -0.0902, -0.1137,  0.0784, -0.0808, -0.1679,  0.0889, -0.0004,\n",
            "          0.0668,  0.1646, -0.0223, -0.0834, -0.1494, -0.0070, -0.1294, -0.0352,\n",
            "          0.0622, -0.1425,  0.1193,  0.0865, -0.1587,  0.0923,  0.1170,  0.0891],\n",
            "        [-0.0044, -0.0755,  0.1037, -0.1024, -0.0993,  0.0316,  0.0754,  0.0454,\n",
            "         -0.1557, -0.0186, -0.0395,  0.1132,  0.1217, -0.0411,  0.1296,  0.0806,\n",
            "         -0.1392,  0.0379, -0.1415,  0.0382, -0.1388, -0.0404,  0.0885,  0.1257,\n",
            "         -0.0038,  0.1683,  0.0022,  0.0651,  0.0908, -0.1726,  0.0441, -0.0026],\n",
            "        [-0.0289,  0.1372, -0.0070, -0.1454, -0.0651,  0.0849,  0.1358, -0.1549,\n",
            "         -0.1516,  0.0825, -0.0370, -0.0335, -0.1228, -0.0138, -0.1578,  0.0701,\n",
            "          0.1761, -0.1184, -0.0774,  0.0176,  0.0102,  0.1469,  0.0546, -0.1312,\n",
            "          0.1275,  0.1023, -0.0327,  0.1287, -0.0989,  0.1579,  0.0927,  0.1720],\n",
            "        [ 0.0408,  0.1596,  0.0412,  0.0405, -0.0835, -0.1498,  0.1175,  0.1315,\n",
            "          0.0782,  0.0315, -0.0496, -0.0590, -0.0396,  0.1208,  0.0745,  0.1226,\n",
            "          0.1577, -0.1604, -0.1635,  0.1533, -0.0720,  0.1042, -0.0200,  0.0634,\n",
            "          0.0871,  0.1655, -0.1639, -0.1672,  0.1305,  0.0942,  0.0737,  0.0099],\n",
            "        [-0.0037, -0.1471,  0.1535,  0.0368,  0.1582, -0.1455, -0.1479, -0.1262,\n",
            "          0.0275, -0.0853,  0.0841, -0.1617, -0.0029, -0.1499, -0.0819, -0.1117,\n",
            "         -0.1344, -0.1280, -0.0843,  0.1428,  0.0846, -0.1414, -0.0489, -0.0810,\n",
            "         -0.0574, -0.0341,  0.1338, -0.0948, -0.0126, -0.1122,  0.1213,  0.1519],\n",
            "        [-0.0056,  0.1465,  0.0831, -0.1393, -0.0832,  0.0334, -0.0553,  0.0156,\n",
            "         -0.0252,  0.0858,  0.0665,  0.0622,  0.1006,  0.1373, -0.0169,  0.0379,\n",
            "         -0.0373, -0.0014, -0.0173,  0.1707,  0.1742, -0.0862,  0.1309, -0.0339,\n",
            "          0.0542,  0.1138, -0.0033,  0.0899, -0.1571,  0.0364,  0.0752, -0.0196],\n",
            "        [ 0.0322,  0.1561,  0.1130,  0.1608,  0.1475, -0.0390,  0.0569,  0.1059,\n",
            "          0.1713, -0.1374, -0.1287, -0.0603,  0.1479,  0.1235, -0.0113,  0.0326,\n",
            "          0.0651,  0.1287,  0.1732, -0.1171,  0.1604,  0.0695, -0.1050,  0.1060,\n",
            "          0.0950,  0.1735,  0.0131, -0.1424, -0.1427,  0.0720,  0.1432, -0.1562],\n",
            "        [-0.0187, -0.0967,  0.1666,  0.0518, -0.0662, -0.0477, -0.1148,  0.1117,\n",
            "         -0.1207,  0.0976, -0.0873, -0.0173,  0.1304,  0.0708, -0.0375,  0.0767,\n",
            "          0.1707, -0.1267,  0.0255,  0.0624, -0.0700, -0.0081,  0.0273,  0.1207,\n",
            "         -0.1428, -0.0236, -0.1350, -0.0251,  0.0895,  0.0229,  0.1085,  0.0339],\n",
            "        [ 0.0105, -0.1084, -0.0631, -0.0752,  0.0939, -0.0296,  0.0850, -0.0675,\n",
            "          0.0456,  0.0386,  0.1577,  0.0238, -0.0242, -0.1414,  0.1310,  0.0650,\n",
            "         -0.1692,  0.0251,  0.0831, -0.0978,  0.0573,  0.0262,  0.1217,  0.1706,\n",
            "         -0.0791,  0.0290, -0.0426, -0.1454, -0.0696, -0.1706,  0.1162,  0.1304],\n",
            "        [ 0.1303, -0.0678, -0.1023,  0.1204, -0.0839,  0.0637, -0.0295, -0.0849,\n",
            "          0.0283,  0.1047,  0.0433, -0.0907, -0.1249,  0.0938,  0.0099, -0.0781,\n",
            "          0.0120, -0.1212, -0.0412, -0.0519, -0.1671, -0.1410, -0.0088,  0.1139,\n",
            "         -0.1583, -0.1085, -0.0378,  0.1237,  0.1082,  0.1748,  0.0872, -0.0227],\n",
            "        [ 0.0779, -0.0989, -0.1298, -0.1518, -0.1636, -0.1276,  0.0719,  0.1162,\n",
            "         -0.0490, -0.0135, -0.0115, -0.1719,  0.1765,  0.0245,  0.0266,  0.1112,\n",
            "         -0.1474, -0.1195, -0.0225, -0.0398, -0.1326, -0.1255, -0.1229, -0.1618,\n",
            "          0.0298,  0.0211, -0.0908, -0.0654, -0.1510,  0.0936,  0.1035, -0.0524],\n",
            "        [-0.0563,  0.1523, -0.1199, -0.0845,  0.0251, -0.0802,  0.0762, -0.0979,\n",
            "         -0.1663,  0.0251, -0.0609, -0.1105,  0.1054, -0.0285,  0.0433, -0.0312,\n",
            "          0.0415,  0.0369, -0.0120,  0.0233, -0.1596, -0.0428, -0.1150,  0.1406,\n",
            "          0.1241,  0.0710,  0.1474,  0.1046, -0.1099,  0.0113,  0.0590, -0.0214],\n",
            "        [-0.1079, -0.0023, -0.0874,  0.1551, -0.1308, -0.1343,  0.1634, -0.1650,\n",
            "         -0.1198, -0.0091,  0.0037, -0.1656,  0.1274,  0.0363,  0.1154,  0.1253,\n",
            "          0.1495, -0.0881,  0.1476, -0.1168, -0.1162,  0.0407, -0.1071,  0.0849,\n",
            "          0.1203,  0.1217, -0.0004, -0.1642,  0.0099, -0.0165,  0.0313, -0.1012],\n",
            "        [ 0.1560, -0.1154, -0.0132,  0.0888,  0.1132, -0.0372, -0.1494,  0.1123,\n",
            "          0.0272, -0.1595,  0.1116, -0.0402, -0.1618,  0.0370, -0.1002,  0.0599,\n",
            "          0.0430,  0.1623, -0.1747,  0.1233, -0.0737, -0.1756, -0.0078, -0.1015,\n",
            "          0.0745, -0.1428, -0.0269, -0.0933, -0.1651, -0.0086,  0.0652, -0.1340],\n",
            "        [-0.1327,  0.1571,  0.1645,  0.1688,  0.0683,  0.1710, -0.0127, -0.0679,\n",
            "         -0.1006,  0.1439, -0.1362, -0.1320,  0.0369, -0.0743,  0.1039, -0.0540,\n",
            "          0.0410,  0.0739, -0.1145, -0.0709, -0.0787,  0.0681,  0.0547,  0.1479,\n",
            "         -0.1412, -0.0312,  0.0821, -0.1250,  0.0878,  0.0874, -0.1329,  0.0078]])\n",
            "  bias:   tensor([-0.0682, -0.0433, -0.1462,  0.0968, -0.1091,  0.0890,  0.0818, -0.0920,\n",
            "        -0.0932,  0.0262, -0.0327, -0.1041,  0.1380, -0.1094,  0.1684,  0.1311])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2\n",
        "class SharedMLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    shared = nn.LazyLinear(8)\n",
        "    self.net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    nn.LazyLinear(1))\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.net(X)"
      ],
      "metadata": {
        "id": "KAQAPTpSkXzl"
      },
      "id": "KAQAPTpSkXzl",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = SharedMLP()\n",
        "X = torch.rand(20, 20)         # input\n",
        "y = torch.rand(20, 1)          # target\n",
        "\n",
        "criterion = nn.MSELoss()       # loss function\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = net(X)            # forward pass\n",
        "    loss = criterion(y_pred, y)\n",
        "    loss.backward()            # backward pass\n",
        "    optimizer.step()           # update weights\n",
        "\n",
        "    print(f\"Epoch {epoch}: loss = {loss.item():.4f} --> {[(name, param.shape) for name, param in net.named_parameters()]}\")"
      ],
      "metadata": {
        "id": "5Og8MUr8jh2-",
        "outputId": "ddf49e75-5603-4627-ba92-6bc5182139dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5Og8MUr8jh2-",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss = 0.9186 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 1: loss = 0.8746 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 2: loss = 0.8328 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 3: loss = 0.7933 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 4: loss = 0.7558 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 5: loss = 0.7202 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 6: loss = 0.6865 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 7: loss = 0.6544 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 8: loss = 0.6240 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 9: loss = 0.5950 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 10: loss = 0.5675 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 11: loss = 0.5414 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 12: loss = 0.5166 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 13: loss = 0.4930 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 14: loss = 0.4706 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 15: loss = 0.4493 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 16: loss = 0.4291 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 17: loss = 0.4099 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 18: loss = 0.3916 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 19: loss = 0.3744 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 20: loss = 0.3579 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 21: loss = 0.3424 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 22: loss = 0.3276 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 23: loss = 0.3136 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 24: loss = 0.3003 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 25: loss = 0.2877 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 26: loss = 0.2758 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 27: loss = 0.2646 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 28: loss = 0.2540 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 29: loss = 0.2440 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 30: loss = 0.2345 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 31: loss = 0.2255 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 32: loss = 0.2170 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 33: loss = 0.2090 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 34: loss = 0.2015 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 35: loss = 0.1944 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 36: loss = 0.1877 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 37: loss = 0.1813 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 38: loss = 0.1754 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 39: loss = 0.1697 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 40: loss = 0.1645 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 41: loss = 0.1595 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 42: loss = 0.1548 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 43: loss = 0.1504 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 44: loss = 0.1462 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 45: loss = 0.1423 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 46: loss = 0.1387 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 47: loss = 0.1353 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 48: loss = 0.1321 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 49: loss = 0.1291 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 50: loss = 0.1262 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 51: loss = 0.1236 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 52: loss = 0.1211 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 53: loss = 0.1188 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 54: loss = 0.1167 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 55: loss = 0.1147 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 56: loss = 0.1128 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 57: loss = 0.1110 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 58: loss = 0.1094 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 59: loss = 0.1079 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 60: loss = 0.1064 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 61: loss = 0.1051 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 62: loss = 0.1039 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 63: loss = 0.1027 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 64: loss = 0.1016 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 65: loss = 0.1007 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 66: loss = 0.0997 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 67: loss = 0.0989 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 68: loss = 0.0981 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 69: loss = 0.0973 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 70: loss = 0.0967 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 71: loss = 0.0960 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 72: loss = 0.0954 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 73: loss = 0.0949 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 74: loss = 0.0944 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 75: loss = 0.0939 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 76: loss = 0.0935 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 77: loss = 0.0931 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 78: loss = 0.0927 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 79: loss = 0.0923 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 80: loss = 0.0920 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 81: loss = 0.0917 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 82: loss = 0.0914 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 83: loss = 0.0912 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 84: loss = 0.0910 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 85: loss = 0.0907 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 86: loss = 0.0905 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 87: loss = 0.0904 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 88: loss = 0.0902 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 89: loss = 0.0900 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 90: loss = 0.0899 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 91: loss = 0.0897 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 92: loss = 0.0896 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 93: loss = 0.0895 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 94: loss = 0.0894 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 95: loss = 0.0893 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 96: loss = 0.0892 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 97: loss = 0.0891 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 98: loss = 0.0890 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n",
            "Epoch 99: loss = 0.0890 --> [('net.0.weight', torch.Size([8, 20])), ('net.0.bias', torch.Size([8])), ('net.2.weight', torch.Size([8, 8])), ('net.2.bias', torch.Size([8])), ('net.6.weight', torch.Size([1, 8])), ('net.6.bias', torch.Size([1]))]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}